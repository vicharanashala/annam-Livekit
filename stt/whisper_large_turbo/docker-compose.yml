services:
  whisper-server:
    # Use the active repository 'speaches' instead of 'fedirz'
    image: fedirz/faster-whisper-server:latest-cuda
    container_name: whisper_server
    ports:
      - "8030:8000"
    volumes:
      # Map the cache so you don't re-download the 2GB+ model every restart
      - ./hf_cache_local:/root/.cache/huggingface
    environment:
      # The specific CT2 converted model
      - WHISPER__MODEL=deepdml/faster-whisper-large-v3-turbo-ct2
      # Use int8_float16 for compatibility and good speed on GPU
      - WHISPER__COMPUTE_TYPE=int8_float16
      # Ensure it uses the GPU
      - WHISPER__DEVICE=cuda
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
