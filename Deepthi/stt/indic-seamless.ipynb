{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34053ec5-2d5a-4a61-b0c2-a63d51176599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ai4bharat/indic-seamless \n",
    "# ...........................................\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import AutoProcessor, SeamlessM4Tv2ForSpeechToText\n",
    "\n",
    "# -------------------------\n",
    "# Settings\n",
    "# -------------------------\n",
    "MODEL_ID = \"ai4bharat/indic-seamless\"\n",
    "AUDIO_PATH = \"Deepthi/STT/sound5_male.wav\n",
    "OUTPUT_TXT = \"output.txt\"\n",
    "TARGET_LANG = \"mal\"   # Malayalam\n",
    "\n",
    "# Choose decoding mode:\n",
    "# \"deterministic\"  -> Option 1\n",
    "# \"beam\"           -> Option 2\n",
    "DECODE_MODE = \"beam\"\n",
    "\n",
    "# -------------------------\n",
    "# Device\n",
    "# -------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# -------------------------\n",
    "# Load model + processor\n",
    "# -------------------------\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "model = SeamlessM4Tv2ForSpeechToText.from_pretrained(MODEL_ID).to(device)\n",
    "\n",
    "# -------------------------\n",
    "# Load audio\n",
    "# -------------------------\n",
    "waveform, sample_rate = torchaudio.load(AUDIO_PATH)\n",
    "\n",
    "# Convert to mono if stereo\n",
    "if waveform.shape[0] > 1:\n",
    "    waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "# Resample to 16kHz\n",
    "if sample_rate != 16000:\n",
    "    waveform = torchaudio.functional.resample(\n",
    "        waveform,\n",
    "        orig_freq=sample_rate,\n",
    "        new_freq=16000\n",
    "    )\n",
    "\n",
    "waveform = waveform.squeeze()\n",
    "\n",
    "# -------------------------\n",
    "# Preprocess\n",
    "# -------------------------\n",
    "inputs = processor(\n",
    "    audio=waveform.numpy(),\n",
    "    sampling_rate=16000,\n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "\n",
    "# -------------------------\n",
    "# Generate transcription\n",
    "# -------------------------\n",
    "with torch.no_grad():\n",
    "\n",
    "    if DECODE_MODE == \"deterministic\":\n",
    "        # âœ… OPTION 1: Deterministic decoding (more acoustic, less smoothing)\n",
    "        generated_tokens = model.generate(\n",
    "            **inputs,\n",
    "            tgt_lang=TARGET_LANG,\n",
    "            temperature=0.0,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "    elif DECODE_MODE == \"beam\":\n",
    "        # âœ… OPTION 2: Beam search (more accurate than greedy)\n",
    "        generated_tokens = model.generate(\n",
    "            **inputs,\n",
    "            tgt_lang=TARGET_LANG,\n",
    "            num_beams=5,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        # Default (original behaviour)\n",
    "        generated_tokens = model.generate(\n",
    "            **inputs,\n",
    "            tgt_lang=TARGET_LANG\n",
    "        )\n",
    "\n",
    "transcription = processor.batch_decode(\n",
    "    generated_tokens,\n",
    "    skip_special_tokens=True\n",
    ")[0]\n",
    "\n",
    "# -------------------------\n",
    "# Print + Save to TXT\n",
    "# -------------------------\n",
    "print(\"\\nTranscription:\\n\", transcription)\n",
    "\n",
    "with open(OUTPUT_TXT, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(transcription)\n",
    "\n",
    "print(\"\\nSaved to:\", OUTPUT_TXT)\n",
    "\n",
    "# ................................\n",
    "# ==========================================================\n",
    "# ðŸ”¥ LATENCY MEASUREMENT BLOCK (Add at End of Script)\n",
    "# ==========================================================\n",
    "import time\n",
    "\n",
    "print(\"\\n----- Latency Analysis -----\")\n",
    "\n",
    "# Calculate audio duration\n",
    "audio_duration = waveform.shape[0] / 16000\n",
    "print(f\"Audio Duration: {audio_duration:.2f} seconds\")\n",
    "\n",
    "# Synchronize GPU before timing (important for CUDA)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    if DECODE_MODE == \"deterministic\":\n",
    "        generated_tokens = model.generate(\n",
    "            **inputs,\n",
    "            tgt_lang=TARGET_LANG,\n",
    "            temperature=0.0,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "    elif DECODE_MODE == \"beam\":\n",
    "        generated_tokens = model.generate(\n",
    "            **inputs,\n",
    "            tgt_lang=TARGET_LANG,\n",
    "            num_beams=8,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        generated_tokens = model.generate(\n",
    "            **inputs,\n",
    "            tgt_lang=TARGET_LANG\n",
    "        )\n",
    "\n",
    "# Synchronize GPU after inference\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "inference_time = end_time - start_time\n",
    "rtf = inference_time / audio_duration\n",
    "\n",
    "print(f\"Inference Time: {inference_time:.3f} seconds\")\n",
    "print(f\"Real-Time Factor (RTF): {rtf:.3f}\")\n",
    "\n",
    "if rtf < 1:\n",
    "    print(\"Model runs faster than real-time âœ…\")\n",
    "else:\n",
    "    print(\"Model runs slower than real-time âš ï¸\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
